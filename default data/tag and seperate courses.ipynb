{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3adbfc7",
   "metadata": {},
   "source": [
    "# Python notebook that is used to get all the default data from the list of courses with department and difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a319a",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install torch\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66b23d",
   "metadata": {},
   "source": [
    "### nltk requires that these modules be downloaded separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1caccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ec74d",
   "metadata": {},
   "source": [
    "## Read the file and store them in a pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7dc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"list of courses with dept and difficulty.json\")\n",
    "df.rename(columns={\"Offering Dept\": \"degree_type\", \"Difficulty\": \"difficulty\", \"Course Title\": \"course_name\"}, inplace=True)\n",
    "df[\"degree_type\"] = df[\"degree_type\"].str.lower()\n",
    "df[\"course_name\"] = df[\"course_name\"].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb7bda",
   "metadata": {},
   "source": [
    "### Get all the unique degree types and set their index starting from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_types = pd.DataFrame(df[\"degree_type\"].unique(), columns=[\"degree_type\"])\n",
    "degree_types.index = range(1, len(degree_types) + 1)\n",
    "degree_types.index.name = \"id\"\n",
    "degree_types.to_csv(\"degree_type.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06525c1e",
   "metadata": {},
   "source": [
    "### Get all the courses and set their index starting from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "course = df[[\"course_name\", \"difficulty\"]]\n",
    "# remove duplicates\n",
    "course.drop_duplicates(subset=\"course_name\", inplace=True)\n",
    "course.index = range(1, len(course) + 1)\n",
    "course.index.name = \"id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c92f0a",
   "metadata": {},
   "source": [
    "For courses generate embeddings to make content based filtering more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e64d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FacebookAI/xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "course_names = course[\"course_name\"].tolist()\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    course_names,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "token_embeddings = model_output.last_hidden_state\n",
    "\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "mask_expended = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "\n",
    "sum_embeddings = torch.sum(token_embeddings * mask_expended, 1)\n",
    "\n",
    "sum_mask = torch.clamp(mask_expended.sum(1), min=1e-9)\n",
    "sentence_embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "print(\"shape of sentence embeddings:\")\n",
    "print(sentence_embeddings.shape)\n",
    "\n",
    "print(\"\\nEmbedding for the first sentence (first 10 values):\")\n",
    "print(sentence_embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list = sentence_embeddings.cpu().tolist()\n",
    "\n",
    "course['embedding'] = embeddings_list\n",
    "\n",
    "course.to_csv(\"course.csv\")\n",
    "\n",
    "course.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30d483",
   "metadata": {},
   "source": [
    "### Make a junction table which would store the id of degree_type and course so that they can be stored efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7fa0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_with_id = course.reset_index()\\\n",
    "    .rename(columns={\"id\": \"course_id\"})\\\n",
    "    .drop(\"difficulty\", axis=1)\n",
    "\n",
    "degree_with_id = degree_types.reset_index()\\\n",
    "    .rename(columns={\"id\": \"degree_type_id\"})\n",
    "\n",
    "df_modified = df.drop(\"difficulty\", axis=1)\n",
    "\n",
    "course_merged = pd.merge(course_with_id, df_modified, on=\"course_name\", how=\"left\")\n",
    "degree_course = pd.merge(course_merged, degree_with_id, on=\"degree_type\", how=\"left\")\n",
    "\n",
    "degree_course_modified = degree_course.drop([\"degree_type\", \"course_name\"], axis=1)\n",
    "degree_course_modified.to_csv(\"degree_course.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a04d4",
   "metadata": {},
   "source": [
    "### Process of making tags of each course\n",
    "\n",
    "steps to get tags are:\n",
    "\n",
    "1. combine degree name and course name\n",
    "2. lower case the combined strings\n",
    "3. tokenize the combined string\n",
    "4. remove stop words, add some of our own\n",
    "5. lemmatize the words\n",
    "6. return the list of tags for each course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14741155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tags(row) -> list[str]:\n",
    "    degree_type = row[\"degree_type\"]\n",
    "    course_name = row[\"course_name\"]\n",
    "    complete_text = degree_type + \" \" + course_name\n",
    "    \n",
    "    # tokenize the complete text\n",
    "    tokens = word_tokenize(complete_text)\n",
    "\n",
    "    # Define and remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    custom_words = {\"principles\", \"principle\", \"introduction\", \"introductory\", \"intro\", \"of\", \"to\", \"and\", \"in\", \"for\", \"the\", \"with\", \"a\", \"an\", \"i\", \"ii\"}\n",
    "    stop_words.update(custom_words)\n",
    "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    # Lemmatize the remaining words to their root form\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    # list of words to exclude from minimum word length\n",
    "    length_exception = [\"art\", \"ai\", \"ml\", \"war\", \"law\", \"lab\", \"jog\", \"iot\"]\n",
    "\n",
    "    # remove tags that are below the maximum word limit and print them\n",
    "    limit_length_tokens = [word for word in lemmatized_tokens if ((len(word) > 3) or (word in length_exception))]\n",
    "\n",
    "    # use set to remove any duplicate and list to turn into a list again\n",
    "    final_tags = list(set(limit_length_tokens))\n",
    "\n",
    "    return final_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ad0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_tags = degree_course.drop_duplicates(subset=\"course_name\").apply(generate_tags, axis=1)\n",
    "course_tags.index = range(1, len(course_tags) + 1)\n",
    "course_tags.index.name = \"course_id\"\n",
    "course_tags.name = \"course_tags\"\n",
    "course_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2796fc2",
   "metadata": {},
   "source": [
    "## We need to make the tags unique for tag table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff75db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using itertools we will unpack all strings from the course_tag dataframe\n",
    "# putting them in a set will remove all duplicate values and give us unique tags\n",
    "tag = pd.DataFrame(set(it.chain.from_iterable(course_tags)), columns=[\"tag_name\"])\n",
    "tag.index = range(1, len(tag) + 1)\n",
    "tag.index.name = \"id\"\n",
    "tag.to_csv(\"tag.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701d0739",
   "metadata": {},
   "source": [
    "### make course tags which would have id of all the courses linked with their respective tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_with_id = tag.reset_index()\\\n",
    "    .rename(columns={\"id\": \"tag_id\"})\n",
    "\n",
    "course_tags_with_id = course_tags.reset_index()\n",
    "\n",
    "# make the dataframe which would store the id of the tags and courses\n",
    "course_tag = pd.DataFrame(columns=[\"course_id\", \"tag_id\"])\n",
    "\n",
    "for course_id, tags in zip(course_tags_with_id[\"course_id\"], course_tags_with_id[\"course_tags\"]):\n",
    "    for tag_name in tags:\n",
    "        tag_id = tag_with_id.loc[tag_with_id[\"tag_name\"] == tag_name, \"tag_id\"].values[0]\n",
    "        new_row = pd.DataFrame({\"course_id\": [course_id], \"tag_id\": [tag_id]})\n",
    "        course_tag = pd.concat([course_tag, new_row], ignore_index=True)\n",
    "\n",
    "course_tag.to_csv(\"course_tag.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
